# üéØ RedditHarbor CLEAN PIPELINE ARCHITECTURE
## Single Source of Truth - No More Script Sprawl

---

## üö® **CURRENT PROBLEM: Script Sprawl Hell**
- **39+ conflicting scripts** all doing similar work
- **Multiple background processes** corrupting each other's data
- **No clear data ownership** or flow direction
- **Duplicate functionality** causing data inconsistencies

---

## üéØ **CLEAN ARCHITECTURE: Single Pipeline Design**

### **üì¶ CORE COMPONENTS (4 scripts total)**

#### **1. DATA COLLECTION**
**`scripts/collect_reddit_data.py`** - Single collection script
- Reddit API ‚Üí Supabase database
- Configurable subreddit lists
- Single data source truth
- NO opportunity scoring

#### **2. OPPORTUNITY ANALYSIS**
**`scripts/analyze_opportunities.py`** - Single analysis script
- Database ‚Üí LLM profiler (Claude Haiku)
- Quality AI profile generation
- Stores results in `app_opportunities` table
- ONE source of AI-generated content

#### **3. REPORT GENERATION**
**`scripts/generate_reports.py`** - Single reporting script
- Reads from `app_opportunities` table
- Generates database-connected reports
- Financial projections based on real data
- Clean output to `reports/` directory

#### **4. SCHEDULER/AUTOMATION**
**`scripts/run_pipeline.py`** - Single orchestration script
- Coordinates the 3 core scripts
- Runs on schedule (daily/weekly)
- Error handling and monitoring
- Single entry point for automation

---

## üóëÔ∏è **SCRIPTS TO REMOVE (35+ scripts)**

### **DELETE immediately:**
```
automated_reddit_harvester.py
automated_opportunity_collector.py
run_dlt_activity_collection.py
collect_1000_plus_simple_scale.py
collect_ultra_premium_subreddits.py
collect_massive_scale_for_70_plus.py
reddit_mega_scaler.py
full_scale_collection.py
continuous_collection_system.py
collect_high_stakes_subreddits.py

generate_opportunity_insights_openrouter.py
generate_quality_opportunity_reports.py
generate_individual_opportunity_reports.py
generate_ai_profile_reports.py
generate_db_connected_reports.py
e2e_report_pipeline.py
automated_report_scheduler.py

batch_opportunity_scoring.py
score_hunter_60_plus.py
dlt_opportunity_pipeline.py

e2e_test_small_batch.py
e2e_validation_framework.py
stage4_workflow_verification.py
test_dlt_integration.py
final_system_test.py
performance_benchmark.py
qa_function_count_distribution.py
```

### **KEEP (4 core scripts to create):**
```
collect_reddit_data.py
analyze_opportunities.py
generate_reports.py
run_pipeline.py
```

---

## üìä **DATA FLOW DIAGRAM**

```
Reddit API
     ‚Üì
[collect_reddit_data.py]
     ‚Üì
Supabase: submissions table
     ‚Üì
[analyze_opportunities.py]
     ‚Üì
Supabase: app_opportunities table (AI profiles)
     ‚Üì
[generate_reports.py]
     ‚Üì
reports/ directory (final output)
     ‚Üë
[run_pipeline.py] (orchestrates all)
```

---

## üéØ **BENEFITS**

1. **Single Source of Truth**: No conflicting data sources
2. **Clear Data Flow**: One-way pipeline, no loops
3. **Easy Debugging**: 4 scripts vs 39+ scripts
4. **No Data Corruption**: One process at a time
5. **Maintainable**: Clear separation of concerns
6. **Testable**: Each component isolated

---

## üîß **IMPLEMENTATION PLAN**

### **Phase 1: STOP CORRUPTION**
‚úÖ Done - Killed all background processes

### **Phase 2: CREATE CLEAN SCRIPTS**
- [ ] Build `collect_reddit_data.py`
- [ ] Build `analyze_opportunities.py`
- [ ] Build `generate_reports.py`
- [ ] Build `run_pipeline.py`

### **Phase 3: CLEAN UP**
- [ ] Delete 35+ redundant scripts
- [ ] Update documentation
- [ ] Test end-to-end flow

### **Phase 4: VALIDATE**
- [ ] Run clean pipeline
- [ ] Verify output quality
- [ ] Monitor for issues

---

## üöÄ **NEXT STEPS**

1. **Create the 4 core clean scripts**
2. **Test with existing data**
3. **Delete redundant scripts**
4. **Establish single pipeline as source of truth**

**Result: From 39 chaotic scripts ‚Üí 4 clean, focused scripts**