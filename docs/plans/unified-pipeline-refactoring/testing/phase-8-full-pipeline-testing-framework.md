# Phase 8: Full Pipeline Testing Framework

**Date**: 2025-11-20
**Status**: Planning
**Target**: Prove unified OpportunityPipeline produces identical AI-enriched profiles as monolith, ready for production

---

## Executive Summary

### Testing Target

**PRIMARY GOAL**: Validate unified OpportunityPipeline produces **functionally equivalent** AI-enriched profiles as monolith scripts before building FastAPI backend (Phase 9).

**SUCCESS DEFINITION**:
- âœ… **Functional Equivalence**: 95%+ field match rate vs monolith (30+ enrichment fields)
- âœ… **Production Readiness**: 95%+ success rate at scale (200 submissions)
- âœ… **Performance Acceptable**: Processing time within 20% of monolith
- âœ… **Budget Sustainable**: Average cost < $0.15 per submission
- âœ… **Storage Validated**: DLT merge disposition working correctly
- âœ… **Observability Working**: AgentOps, LiteLLM tracking all AI service calls

**WHEN COMPLETE**: All success criteria met â†’ Proceed to Phase 9 (FastAPI Backend)

---

## Naming Conventions

### Directory Structure Naming

```
scripts/testing/integration/                     # Integration testing suite
â”œâ”€â”€ README.md                                    # Suite overview
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ test_config.py                          # Centralized test configuration
â”‚   â”œâ”€â”€ submissions_single.json                 # 1 high-quality submission
â”‚   â”œâ”€â”€ submissions_small_batch.json            # 5 varied submissions
â”‚   â”œâ”€â”€ submissions_monolith_comparison.json    # 10 for monolith comparison
â”‚   â”œâ”€â”€ submissions_medium_batch.json           # 50 for scale testing
â”‚   â”œâ”€â”€ submissions_large_batch.json            # 200 for stress testing
â”‚   â””â”€â”€ service_config.json                     # Service enable/disable flags
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_01_single_submission.py            # Single submission validation
â”‚   â”œâ”€â”€ test_02_small_batch.py                  # 5 submissions, all services
â”‚   â”œâ”€â”€ test_03_monolith_equivalence.py         # Compare vs real monolith (10)
â”‚   â”œâ”€â”€ test_04_medium_scale.py                 # 50 submissions
â”‚   â”œâ”€â”€ test_05_large_scale.py                  # 200 submissions (stress)
â”‚   â”œâ”€â”€ test_06_error_recovery.py               # Service failure scenarios
â”‚   â”œâ”€â”€ test_07_storage_validation.py           # DLT storage integration
â”‚   â”œâ”€â”€ test_08_cost_monitoring.py              # Cost tracking and budgets
â”‚   â””â”€â”€ test_09_observability.py                # AgentOps/LiteLLM validation
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ comparison.py                           # Field-by-field comparison logic
â”‚   â”œâ”€â”€ metrics.py                              # Metrics collection
â”‚   â”œâ”€â”€ reporting.py                            # JSON/console report generation
â”‚   â”œâ”€â”€ monolith_runner.py                      # Helper to run real monolith
â”‚   â””â”€â”€ observability.py                        # AgentOps/LiteLLM helpers
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ test_01_single_submission/
â”‚   â”‚   â”œâ”€â”€ run_2025-11-20_10-30-00.json
â”‚   â”‚   â””â”€â”€ run_2025-11-20_14-45-00.json
â”‚   â”œâ”€â”€ test_02_small_batch/
â”‚   â”œâ”€â”€ test_03_monolith_equivalence/
â”‚   â”‚   â”œâ”€â”€ unified_results_2025-11-20.json
â”‚   â”‚   â”œâ”€â”€ monolith_results_2025-11-20.json
â”‚   â”‚   â””â”€â”€ comparison_report_2025-11-20.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ monolith_baseline/
â”‚   â”œâ”€â”€ generate_baseline.sh                    # Script to run monolith and store results
â”‚   â””â”€â”€ results/
â”‚       â””â”€â”€ monolith_enrichment_2025-11-20.json
â”‚
â””â”€â”€ observability/
    â”œâ”€â”€ agentops_traces/                        # AgentOps session exports
    â”œâ”€â”€ litellm_logs/                           # LiteLLM cost tracking logs
    â””â”€â”€ agno_traces/                            # Agno agent execution traces
```

### Documentation Naming

```
docs/plans/unified-pipeline-refactoring/
â”‚
â”œâ”€â”€ PHASE-8-FULL-PIPELINE-TESTING-FRAMEWORK.md  # This document (overview)
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ integration-testing/
â”‚       â”œâ”€â”€ test-01-single-submission-prompt.md
â”‚       â”œâ”€â”€ test-02-small-batch-prompt.md
â”‚       â”œâ”€â”€ test-03-monolith-equivalence-prompt.md
â”‚       â”œâ”€â”€ test-04-medium-scale-prompt.md
â”‚       â”œâ”€â”€ test-05-large-scale-prompt.md
â”‚       â”œâ”€â”€ test-06-error-recovery-prompt.md
â”‚       â”œâ”€â”€ test-07-storage-validation-prompt.md
â”‚       â”œâ”€â”€ test-08-cost-monitoring-prompt.md
â”‚       â””â”€â”€ test-09-observability-prompt.md
â”‚
â””â”€â”€ local-ai-report/
    â””â”€â”€ integration-testing/
        â”œâ”€â”€ test-01-single-submission-report.md
        â”œâ”€â”€ test-02-small-batch-report.md
        â”œâ”€â”€ test-03-monolith-equivalence-report.md
        â”œâ”€â”€ test-04-medium-scale-report.md
        â”œâ”€â”€ test-05-large-scale-report.md
        â”œâ”€â”€ test-06-error-recovery-report.md
        â”œâ”€â”€ test-07-storage-validation-report.md
        â”œâ”€â”€ test-08-cost-monitoring-report.md
        â””â”€â”€ test-09-observability-report.md
```

### File Naming Convention Rules

1. **Test Scripts**: `test_{number}_{descriptive_name}.py`
   - `test_01_` = Single submission validation
   - `test_02_` = Small batch (5)
   - `test_03_` = Monolith equivalence (critical comparison)
   - `test_04_` = Medium scale (50)
   - `test_05_` = Large scale (200)
   - `test_06_` = Error recovery
   - `test_07_` = Storage validation
   - `test_08_` = Cost monitoring
   - `test_09_` = Observability validation

2. **Config Files**: `{entity}_{size/type}.json`
   - `submissions_single.json` = 1 submission
   - `submissions_small_batch.json` = 5 submissions
   - `submissions_monolith_comparison.json` = 10 submissions
   - `submissions_medium_batch.json` = 50 submissions
   - `submissions_large_batch.json` = 200 submissions

3. **Results**: `run_{ISO_timestamp}.json`
   - `run_2025-11-20_10-30-00.json` = Timestamped run
   - Stored in test-specific subdirectories

4. **Prompts**: `test-{number}-{name}-prompt.md`
   - Matches test script naming
   - Easy to find corresponding prompt for each test

5. **Reports**: `test-{number}-{name}-report.md`
   - Matches test script and prompt naming
   - Clear traceability

---

## Clear Path Forward

### Testing Roadmap (Sequential Execution)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 8: FULL PIPELINE INTEGRATION TESTING                         â”‚
â”‚  Target: Prove unified pipeline = monolith (functionally)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 01: Single Submission Validation                              â”‚
â”‚  Goal: Prove all AI services execute and populate enrichment fields â”‚
â”‚  Command: test_01_single_submission.py --submission-id sub_001      â”‚
â”‚  Duration: 30 mins (including fixes)                                â”‚
â”‚  Success: All 30+ fields populated, no crashes                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 02: Small Batch (5 Submissions)                               â”‚
â”‚  Goal: Prove consistency across multiple submissions                â”‚
â”‚  Command: test_02_small_batch.py                                    â”‚
â”‚  Duration: 1 hour                                                   â”‚
â”‚  Success: 5/5 processed, 100% field coverage, cost < $1             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 03: Monolith Equivalence (10 Submissions) â­ CRITICAL          â”‚
â”‚  Goal: Prove unified pipeline = monolith (field-by-field)           â”‚
â”‚  Steps:                                                              â”‚
â”‚    1. Run real monolith scripts on 10 submissions                   â”‚
â”‚    2. Store monolith results in baseline                            â”‚
â”‚    3. Run unified pipeline on same 10 submissions                   â”‚
â”‚    4. Compare 30+ fields with tolerances                            â”‚
â”‚  Duration: 3-4 hours                                                â”‚
â”‚  Success: 95%+ field match rate                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 04: Medium Scale (50 Submissions)                             â”‚
â”‚  Goal: Validate production batch size performance                   â”‚
â”‚  Command: test_04_medium_scale.py                                   â”‚
â”‚  Duration: 2-3 hours                                                â”‚
â”‚  Success: 48+/50 (96%+), performance within 20% of monolith         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 05: Large Scale (200 Submissions) - Stress Test               â”‚
â”‚  Goal: Prove production readiness at scale                          â”‚
â”‚  Command: test_05_large_scale.py                                    â”‚
â”‚  Duration: 8-10 hours                                               â”‚
â”‚  Success: 190+/200 (95%+), no memory leaks, cost < $30              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 06: Error Recovery                                            â”‚
â”‚  Goal: Validate graceful degradation                                â”‚
â”‚  Command: test_06_error_recovery.py                                 â”‚
â”‚  Duration: 2 hours                                                  â”‚
â”‚  Success: Pipeline continues despite service failures               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 07: Storage Validation                                        â”‚
â”‚  Goal: Validate DLT storage layer integration                       â”‚
â”‚  Command: test_07_storage_validation.py                             â”‚
â”‚  Duration: 1 hour                                                   â”‚
â”‚  Success: DLT merge disposition working, no duplicates              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 08: Cost Monitoring                                           â”‚
â”‚  Goal: Validate budget sustainability                               â”‚
â”‚  Command: test_08_cost_monitoring.py                                â”‚
â”‚  Duration: 1 hour                                                   â”‚
â”‚  Success: Average cost < $0.15/submission                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEST 09: Observability Validation                                  â”‚
â”‚  Goal: Validate AgentOps/LiteLLM tracking                           â”‚
â”‚  Command: test_09_observability.py                                  â”‚
â”‚  Duration: 2 hours                                                  â”‚
â”‚  Success: All AI calls tracked, traces exportable                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸŽ¯ PHASE 8 COMPLETE: UNIFIED PIPELINE VALIDATED                    â”‚
â”‚  âœ… Functional equivalence proven                                    â”‚
â”‚  âœ… Production readiness validated                                   â”‚
â”‚  âœ… Observability working                                            â”‚
â”‚  â†’ PROCEED TO PHASE 9: FASTAPI BACKEND                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Total Estimated Timeline

**Aggressive**: 1 week (if no major issues)
**Conservative**: 2 weeks (with expected debugging/fixes)

---

## Observability Integration Strategy

### Tools to Integrate

1. **AgentOps** - AI agent observability and monitoring
2. **LiteLLM** - Unified LLM interface with cost tracking
3. **Agno** - Multi-agent monetization analysis framework

### Where Each Tool Fits

#### 1. AgentOps Integration

**Purpose**: Track all AI agent executions, costs, latency, errors

**Integration Points**:
- **ProfilerService** (`core/enrichment/profiler_service.py`)
  - Track EnhancedLLMProfiler calls
  - Monitor Claude Haiku API latency
  - Track profiling costs

- **MonetizationService** (`core/enrichment/monetization_service.py`)
  - Track Agno multi-agent team executions
  - Monitor individual agent traces
  - Track monetization analysis costs ($0.10 per call)

- **MarketValidationService** (`core/enrichment/market_validation_service.py`)
  - Track MarketDataValidator calls
  - Monitor Jina AI API calls
  - Track web scraping operations

**Implementation**:
```python
# core/enrichment/profiler_service.py
import agentops

class ProfilerService(BaseEnrichmentService):
    def __init__(self, config):
        super().__init__(config)
        # Initialize AgentOps session
        self.agentops_client = agentops.Client(api_key=os.getenv("AGENTOPS_API_KEY"))
        self.session = self.agentops_client.start_session(
            tags=["profiler", "enrichment"]
        )

    def enrich(self, submission):
        # Track enrichment operation
        with self.agentops_client.create_agent(
            name="EnhancedLLMProfiler",
            agent_id=f"profiler_{submission['submission_id']}"
        ) as agent:
            # Execute profiling
            result = self.profiler.generate_profile(...)

            # Record metrics
            agent.record(
                event_type="llm_call",
                model="claude-3-haiku",
                cost=0.005,
                latency=2.5,
                result=result
            )
```

**Testing**: Test 09 validates AgentOps integration
- Verify sessions created for each test run
- Verify all AI calls tracked
- Verify cost/latency metrics captured
- Verify traces exportable for analysis

#### 2. LiteLLM Integration

**Purpose**: Unified LLM interface with automatic cost tracking across providers

**Integration Points**:
- **ProfilerService**: Route Claude calls through LiteLLM
- **MonetizationService**: Route OpenAI/Anthropic calls through LiteLLM
- **All AI Services**: Centralized cost tracking

**Implementation**:
```python
# core/enrichment/profiler_service.py
import litellm

class ProfilerService(BaseEnrichmentService):
    def enrich(self, submission):
        # Use LiteLLM for unified interface
        response = litellm.completion(
            model="claude-3-haiku-20240307",
            messages=[{"role": "user", "content": prompt}],
            api_base=os.getenv("OPENROUTER_BASE_URL"),
            api_key=os.getenv("OPENROUTER_API_KEY"),
            # LiteLLM automatically tracks cost
        )

        # Get cost from LiteLLM
        cost = litellm.completion_cost(
            completion_response=response,
            model="claude-3-haiku-20240307"
        )

        self.stats["total_cost"] += cost
```

**Benefits**:
- Automatic cost calculation across all providers
- Unified error handling
- Automatic retries with exponential backoff
- Rate limit handling
- Cost analytics and reporting

**Testing**: Test 08 validates LiteLLM cost tracking
- Verify costs calculated correctly for each provider
- Verify total costs match expected budget
- Verify cost breakdown by service
- Export cost reports for analysis

#### 3. Agno Integration

**Purpose**: Multi-agent monetization analysis (already integrated)

**Current Integration**:
- `core/agents/monetization/factory.py` - Creates Agno analyzer
- `core/enrichment/monetization_service.py` - Uses Agno for analysis
- Agno runs multi-agent team:
  - WillingnessToPayAgent
  - PriceSensitivityAgent
  - RevenueOpportunityAgent
  - CustomerSegmentAgent

**What to Test**:
- Verify Agno agents execute correctly through unified pipeline
- Track Agno execution costs (~$0.10 per analysis)
- Monitor Agno agent traces
- Validate deduplication logic (70% cost savings)

**Testing**: Test 09 validates Agno integration
- Verify multi-agent team executes
- Verify all 4 agents contribute
- Verify deduplication working (copied vs fresh analysis)
- Track Agno-specific costs separately

---

## Test Configuration Management

### Service Configuration

**File**: `scripts/testing/integration/config/service_config.json`

```json
{
  "services": {
    "profiler": {
      "enabled": true,
      "config": {
        "model": "claude-3-haiku-20240307",
        "provider": "openrouter",
        "temperature": 0.7,
        "max_tokens": 1000
      },
      "observability": {
        "agentops_enabled": true,
        "litellm_enabled": true
      }
    },
    "opportunity": {
      "enabled": true,
      "config": {
        "scoring_method": "five_dimensional"
      },
      "observability": {
        "agentops_enabled": false,
        "litellm_enabled": false
      }
    },
    "monetization": {
      "enabled": true,
      "config": {
        "use_agno": true,
        "model": "openai/gpt-4o-mini",
        "deduplication_enabled": true
      },
      "observability": {
        "agentops_enabled": true,
        "litellm_enabled": true,
        "agno_tracing_enabled": true
      }
    },
    "trust": {
      "enabled": true,
      "config": {
        "validation_method": "six_dimensional"
      },
      "observability": {
        "agentops_enabled": false,
        "litellm_enabled": false
      }
    },
    "market_validation": {
      "enabled": true,
      "config": {
        "jina_api_key": "${JINA_API_KEY}",
        "max_searches": 3
      },
      "observability": {
        "agentops_enabled": true,
        "litellm_enabled": true
      }
    }
  },
  "storage": {
    "dlt_enabled": true,
    "merge_disposition": true,
    "table_name": "app_opportunities"
  },
  "observability": {
    "agentops": {
      "enabled": true,
      "api_key": "${AGENTOPS_API_KEY}",
      "export_traces": true,
      "export_path": "scripts/testing/integration/observability/agentops_traces"
    },
    "litellm": {
      "enabled": true,
      "log_level": "INFO",
      "export_logs": true,
      "export_path": "scripts/testing/integration/observability/litellm_logs"
    },
    "agno": {
      "enabled": true,
      "export_traces": true,
      "export_path": "scripts/testing/integration/observability/agno_traces"
    }
  }
}
```

### Test Submission Selection

**File**: `scripts/testing/integration/config/submissions_single.json`

```json
{
  "description": "Single high-quality submission for initial validation",
  "selection_criteria": {
    "reddit_score": ">= 50",
    "num_comments": ">= 10",
    "text_length": ">= 200",
    "problem_keywords": ">= 3"
  },
  "submissions": [
    {
      "submission_id": "abc123xyz",
      "title": "I hate manually tracking expenses across multiple bank accounts",
      "subreddit": "productivity",
      "reddit_score": 127,
      "num_comments": 43,
      "reason": "High quality, clear problem, active discussion, good for all services"
    }
  ]
}
```

**File**: `scripts/testing/integration/config/submissions_small_batch.json`

```json
{
  "description": "5 varied submissions for small batch testing",
  "selection_criteria": "Mixed quality levels to test all scenarios",
  "submissions": [
    {
      "submission_id": "high_001",
      "quality": "high",
      "reason": "High quality, all services should execute"
    },
    {
      "submission_id": "medium_001",
      "quality": "medium",
      "reason": "Medium quality, some services may skip"
    },
    {
      "submission_id": "low_001",
      "quality": "low",
      "reason": "Low quality, tests graceful degradation"
    },
    {
      "submission_id": "edge_long_text",
      "quality": "edge_case",
      "reason": "Very long text (>5000 chars), tests text handling"
    },
    {
      "submission_id": "edge_minimal",
      "quality": "edge_case",
      "reason": "Minimal data, tests required field validation"
    }
  ]
}
```

---

## Success Criteria (Detailed)

### Test 01: Single Submission Validation

**Must Pass**:
- âœ… All 5 services execute successfully
- âœ… All 30+ enrichment fields populated
- âœ… Processing time: 15-30 seconds
- âœ… Cost: $0.10-$0.20
- âœ… No unhandled exceptions
- âœ… Data stored in database

**Should Pass**:
- âœ… AgentOps session created
- âœ… LiteLLM costs tracked
- âœ… Agno traces exportable

### Test 02: Small Batch (5 Submissions)

**Must Pass**:
- âœ… 5/5 submissions processed
- âœ… Success rate: 100%
- âœ… Average field coverage: 90%+
- âœ… Total cost: $0.50-$1.00
- âœ… No crashes

**Should Pass**:
- âœ… Performance: 3-5 submissions/minute
- âœ… Memory: < 1GB peak

### Test 03: Monolith Equivalence â­ CRITICAL

**Must Pass**:
- âœ… **Field match rate: 95%+** (30+ fields compared)
- âœ… Numeric fields within tolerance:
  - opportunity_score: Â±1.0
  - final_score: Â±1.0
  - monetization_score: Â±2.0
  - trust_score: Â±2.0
  - market_validation_score: Â±5.0
- âœ… Array fields match (order-independent):
  - core_functions
  - monetization_methods
  - trust_badges
- âœ… Exact match fields:
  - priority (HIGH/MEDIUM/LOW)
  - trust_level (GOLD/SILVER/BRONZE/BASIC)
  - profession

**Should Pass**:
- âœ… Performance within 20% of monolith
- âœ… Cost within 10% of monolith

**This is the GATE to Phase 9** - If this fails, we iterate until it passes.

### Test 04-05: Scale Testing

**Must Pass**:
- âœ… Medium (50): 48+/50 success rate (96%+)
- âœ… Large (200): 190+/200 success rate (95%+)
- âœ… Performance: 3-5 submissions/minute
- âœ… Memory: < 2GB peak
- âœ… No memory leaks

**Should Pass**:
- âœ… Cost averages < $0.15/submission
- âœ… Error rate < 5%

### Test 06: Error Recovery

**Must Pass**:
- âœ… Pipeline continues despite individual service failures
- âœ… Failed services logged clearly
- âœ… Partial enrichment stored
- âœ… Statistics track failures correctly

### Test 07: Storage Validation

**Must Pass**:
- âœ… DLT pipeline stores data correctly
- âœ… Merge disposition prevents duplicates
- âœ… Schema compatibility verified
- âœ… All fields stored with correct data types

### Test 08: Cost Monitoring

**Must Pass**:
- âœ… Average cost/submission: < $0.15
- âœ… Projected monthly cost (10K): < $1000
- âœ… Cost breakdown by service available
- âœ… Deduplication savings tracked

### Test 09: Observability

**Must Pass**:
- âœ… AgentOps sessions created for each run
- âœ… All AI calls tracked in AgentOps
- âœ… LiteLLM costs calculated correctly
- âœ… Agno traces exportable
- âœ… Traces include: model, cost, latency, errors

---

## Exit Criteria for Phase 8

Phase 8 is **COMPLETE** when:

1. âœ… **Tests 01-03 PASS** (proof of concept + monolith equivalence)
2. âœ… **Test 03 achieves 95%+ field match rate** (critical gate)
3. âœ… **Tests 04-05 PASS** (scale testing)
4. âœ… **Tests 06-09 PASS** (error recovery, storage, cost, observability)
5. âœ… **All success criteria met**
6. âœ… **Comprehensive testing report generated**

**THEN**: Proceed to Phase 9 (FastAPI Backend Development)

**IF ANY TEST FAILS**: Iterate, fix, re-test until all pass.

---

## Next Steps

1. **Confirm approach** with user
2. **Create Test 01** (single submission validation)
   - Script: `test_01_single_submission.py`
   - Config: `submissions_single.json`
   - Prompt: `test-01-single-submission-prompt.md`
3. **Submit to local AI** for testing
4. **Review results**, iterate if needed
5. **Proceed to Test 02-09** sequentially

**Estimated Total Duration**: 1-2 weeks
**Target Date**: Phase 8 complete by early December 2025
